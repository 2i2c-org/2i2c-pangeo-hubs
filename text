Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "rook-release" chart repository
...Successfully got an update from the "autoscaler" chart repository
...Successfully got an update from the "grafana" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈
Saving 1 charts
Downloading daskhub from repo https://helm.dask.org
Deleting outdated charts
Release "farallon-staging" has been upgraded. Happy Helming!
NAME: farallon-staging
LAST DEPLOYED: Mon Oct 19 23:56:37 2020
NAMESPACE: farallon-staging
STATUS: pending-upgrade
REVISION: 6
TEST SUITE: None
USER-SUPPLIED VALUES:
daskhub:
  dask-gateway:
    gateway:
      auth:
        jupyterhub:
          apiToken: 95576ebd124867377a5b576bc8cad4eae09fab09c35ffa86dda7df3bae235c50
  jupyterhub:
    auth:
      dummy:
        password: watwatwat
      github:
        callbackUrl: https://staging.farallon.2i2c.cloud/hub/oauth_callback
        clientId: 9c4b51ba2018557f58a3
        clientSecret: bf84479b977e9d2cf69097d175743ce1afb2e731
      state:
        cryptoKey: c669524034563af3deb9001b359336f20e5c2e96e4eefbfa257e42950b41486b
      type: github
    hub:
      services:
        dask-gateway:
          apiToken: 95576ebd124867377a5b576bc8cad4eae09fab09c35ffa86dda7df3bae235c50
    proxy:
      https:
        enabled: true
        hosts:
        - staging.farallon.2i2c.cloud
        letsencrypt:
          contactEmail: yuvipanda@gmail.com
      secretToken: 15ae25fc67559b586199b6f1c6be9ee8629b4891280c867d58602def3ae69b6d
    singleuser:
      image:
        name: 677861182063.dkr.ecr.us-east-2.amazonaws.com/2i2c-hub/user-image
        tag: 4157727a
      initContainers:
      - command:
        - sh
        - -c
        - id && chown 1000:1000 /home/jovyan && ls -lhd /home/jovyan
        image: busybox
        name: volume-mount-hack
        securityContext:
          runAsUser: 0
        volumeMounts:
        - mountPath: /home/jovyan
          name: home
          subPath: homes/farallon/{username}
      storage:
        static:
          pvcName: home-nfs
          subPath: homes/farallon/{username}
        type: static
      workingDir: /home/jovyan
nfsPVC:
  enabled: true
  nfs:
    serverIP: fs-7b129903.efs.us-east-2.amazonaws.com
    shareName: ""

COMPUTED VALUES:
daskhub:
  dask-gateway:
    controller:
      annotations: {}
      backoffBaseDelay: 0.1
      backoffMaxDelay: 300
      completedClusterCleanupPeriod: 600
      completedClusterMaxAge: 86400
      enabled: true
      image:
        name: daskgateway/dask-gateway-server
        pullPolicy: IfNotPresent
        tag: 0.8.0
      k8sApiRateLimit: 50
      k8sApiRateLimitBurst: 100
      loglevel: INFO
      resources: {}
    enabled: true
    gateway:
      annotations: {}
      auth:
        custom:
          options: {}
        jupyterhub:
          apiToken: 95576ebd124867377a5b576bc8cad4eae09fab09c35ffa86dda7df3bae235c50
        kerberos: {}
        simple: {}
        type: jupyterhub
      backend:
        environment: null
        image:
          name: daskgateway/dask-gateway
          pullPolicy: IfNotPresent
          tag: 0.8.0
        namespace: null
        scheduler:
          cores:
            limit: 1
            request: 0.01
          extraContainerConfig: {}
          extraPodConfig:
            tolerations:
            - effect: NoSchedule
              key: k8s.dask.org/dedicated
              operator: Equal
              value: scheduler
            - effect: NoSchedule
              key: k8s.dask.org_dedicated
              operator: Equal
              value: scheduler
          memory:
            limit: 1G
            request: 128M
        worker:
          cores: {}
          extraContainerConfig:
            securityContext:
              runAsGroup: 1000
              runAsUser: 1000
          extraPodConfig:
            nodeSelector:
              dask-pool-name: worker-2xlarge
            securityContext:
              fsGroup: 1000
            tolerations:
            - effect: NoSchedule
              key: dask-dedicated
              operator: Equal
              value: worker
          memory: {}
      extraConfig:
        idle: |
          # timeout after 30 minutes of inactivity
          c.KubeClusterConfig.idle_timeout = 1800
        optionHandler: |
          from dask_gateway_server.options import Options, Integer, Float, String
          def cluster_options(user):
             def option_handler(options):
                 if ":" not in options.image:
                     raise ValueError("When specifying an image you must also provide a tag")
                 extra_annotations = {
                     "hub.jupyter.org/username": user.name,
                     "prometheus.io/scrape": "true",
                     "prometheus.io/port": "8787",
                 }
                 extra_labels = {
                     "hub.jupyter.org/username": user.name,
                 }
                 return {
                     "worker_cores_limit": options.worker_cores,
                     "worker_cores": min(options.worker_cores / 2, 1),
                     "worker_memory": "%fG" % options.worker_memory,
                     "image": options.image,
                     "scheduler_extra_pod_annotations": extra_annotations,
                     "worker_extra_pod_annotations": extra_annotations,
                     "scheduler_extra_pod_labels": extra_labels,
                     "worker_extra_pod_labels": extra_labels,
                 }
             return Options(
                 Integer("worker_cores", 2, min=1, max=16, label="Worker Cores"),
                 Float("worker_memory", 4, min=1, max=32, label="Worker Memory (GiB)"),
                 String("image", default="pangeo/pangeo-notebook:latest", label="Image"),
                 handler=option_handler,
             )
          c.Backend.cluster_options = cluster_options
      image:
        name: daskgateway/dask-gateway-server
        pullPolicy: IfNotPresent
        tag: 0.8.0
      livenessProbe:
        enabled: true
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        timeoutSeconds: 2
      loglevel: INFO
      prefix: /services/dask-gateway
      readinessProbe:
        enabled: true
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 10
        timeoutSeconds: 2
      replicas: 1
      resources: {}
      service:
        annotations: {}
    global: {}
    rbac:
      controller: {}
      enabled: true
      gateway: {}
      traefik: {}
    traefik:
      additionalArguments: []
      affinity: {}
      annotations: {}
      dashboard: false
      image:
        name: traefik
        tag: 2.1.3
      loglevel: WARN
      nodeSelector: {}
      replicas: 1
      resources: {}
      service:
        annotations: {}
        ports:
          tcp:
            port: web
          web:
            port: 80
        spec: {}
        type: ClusterIP
      tolerations: []
  dask-kubernetes:
    enabled: false
  global: {}
  jupyterhub:
    auth:
      admin:
        access: true
      dummy:
        password: watwatwat
      github:
        callbackUrl: https://staging.farallon.2i2c.cloud/hub/oauth_callback
        clientId: 9c4b51ba2018557f58a3
        clientSecret: bf84479b977e9d2cf69097d175743ce1afb2e731
      ldap:
        dn:
          search: {}
          user: {}
        user: {}
      state:
        cryptoKey: c669524034563af3deb9001b359336f20e5c2e96e4eefbfa257e42950b41486b
        enabled: true
      type: github
      whitelist: {}
    cull:
      concurrency: 1
      enabled: true
      every: 1800
      maxAge: 0
      removeNamedServers: false
      timeout: 3600
      users: false
    custom: {}
    debug:
      enabled: false
    global: {}
    hub:
      allowNamedServers: true
      annotations: {}
      baseUrl: /
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      db:
        pvc:
          accessModes:
          - ReadWriteOnce
          annotations: {}
          selector: {}
          storage: 1Gi
        type: sqlite-pvc
      deploymentStrategy:
        type: Recreate
      extraConfig:
        00-add-dask-gateway-values: |
          # 1. Sets `DASK_GATEWAY__PROXY_ADDRESS` in the singleuser environment.
          # 2. Adds the URL for the Dask Gateway JupyterHub service.
          import os

          # These are set by jupyterhub.
          release_name = os.environ["HELM_RELEASE_NAME"]
          release_namespace = os.environ["POD_NAMESPACE"]

          if "PROXY_HTTP_SERVICE_HOST" in os.environ:
              # https is enabled, we want to use the internal http service.
              gateway_address = "http://{}:{}/services/dask-gateway/".format(
                  os.environ["PROXY_HTTP_SERVICE_HOST"],
                  os.environ["PROXY_HTTP_SERVICE_PORT"],
              )
              print("Setting DASK_GATEWAY__ADDRESS {} from HTTP service".format(gateway_address))
          else:
              gateway_address = "http://proxy-public/services/dask-gateway"
              print("Setting DASK_GATEWAY__ADDRESS {}".format(gateway_address))

          # Internal address to connect to the Dask Gateway.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__ADDRESS", gateway_address)
          # Internal address for the Dask Gateway proxy.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__PROXY_ADDRESS", "gateway://traefik-{}-dask-gateway.{}:80".format(release_name, release_namespace))
          # Relative address for the dashboard link.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__PUBLIC_ADDRESS", "/services/dask-gateway/")
          # Use JupyterHub to authenticate with Dask Gateway.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__AUTH__TYPE", "jupyterhub")

          # Adds Dask Gateway as a JupyterHub service to make the gateway available at
          # {HUB_URL}/services/dask-gateway
          service_url = "http://traefik-{}-dask-gateway.{}".format(release_name, release_namespace)
          for service in c.JupyterHub.services:
              if service["name"] == "dask-gateway":
                  if not service.get("url", None):
                      print("Adding dask-gateway service URL")
                      service.setdefault("url", service_url)
                  break
          else:
              print("dask-gateway service not found. Did you set jupyterhub.hub.services.dask-gateway.apiToken?")
        01-working-dir: |
          # Make sure working directory is ${HOME}
          # hubploy has a bug where it unconditionally puts workingdir to be /srv/repo
          c.KubeSpawner.working_dir = '/home/jovyan'
        02-prometheus: |
          # Allow unauthenticated prometheus requests
          # Otherwise our prometheus server can't get to these
          c.JupyterHub.authenticate_prometheus = False
        03-no-setuid: |
          # Disable 'sudo' & similar binaries, regardless of image contents
          c.KubeSpawner.extra_container_config = {
            'securityContext': {
              # Explicitly disallow setuid binaries from working inside the container
              'allowPrivilegeEscalation': False
            }
          }
      extraConfigMap: {}
      extraContainers:
      - args:
        - -c
        - while true; do git fetch origin; git reset --hard origin/master; sleep 5m;
          done
        command:
        - /bin/sh
        image: alpine/git
        name: templates-sync
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /srv/repo
          name: custom-templates
        workingDir: /srv/repo
      extraEnv: {}
      extraVolumeMounts:
      - mountPath: /usr/local/share/jupyterhub/custom_templates
        name: custom-templates
        subPath: templates
      - mountPath: /usr/local/share/jupyterhub/static/extra-assets
        name: custom-templates
        subPath: extra-assets
      extraVolumes:
      - emptyDir: {}
        name: custom-templates
      fsGid: 1000
      image:
        name: jupyterhub/k8s-hub
        tag: 0.9.1
      imagePullSecret:
        enabled: false
      initContainers:
      - args:
        - clone
        - --depth=1
        - --single-branch
        - --
        - https://github.com/2i2c-utoronto/homepage
        - /srv/repo
        image: alpine/git
        name: templates-clone
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /srv/repo
          name: custom-templates
      labels: {}
      livenessProbe:
        enabled: false
        initialDelaySeconds: 30
        periodSeconds: 10
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
        enabled: false
        ingress: []
      nodeSelector:
        jupyterhub-pool-name: core-pool
      pdb:
        enabled: true
        minAvailable: 1
      readinessProbe:
        enabled: false
        initialDelaySeconds: 0
        periodSeconds: 10
      resources:
        limits:
          memory: 2Gi
        requests:
          cpu: 0.001
          memory: 512Mi
      service:
        annotations: {}
        ports: {}
        type: ClusterIP
      services:
        dask-gateway:
          apiToken: 95576ebd124867377a5b576bc8cad4eae09fab09c35ffa86dda7df3bae235c50
      templatePaths: []
      templateVars: {}
      uid: 1000
    ingress:
      annotations: {}
      enabled: false
      hosts: []
      pathSuffix: ""
      tls: []
    prePuller:
      continuous:
        enabled: false
      extraImages: {}
      hook:
        enabled: false
        image:
          name: jupyterhub/k8s-image-awaiter
          tag: 0.9.1
      pause:
        image:
          name: gcr.io/google_containers/pause
          tag: "3.1"
      resources:
        limits:
          cpu: 0.0001
          memory: 32Mi
        requests:
          cpu: 0.0001
          memory: 16Mi
    proxy:
      chp:
        image:
          name: jupyterhub/configurable-http-proxy
          tag: 4.2.1
        livenessProbe:
          enabled: true
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          enabled: true
          initialDelaySeconds: 0
          periodSeconds: 10
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 0.001
            memory: 128Mi
      deploymentStrategy:
        type: Recreate
      https:
        enabled: true
        hosts:
        - staging.farallon.2i2c.cloud
        letsencrypt:
          acmeServer: ""
          contactEmail: yuvipanda@gmail.com
        manual: {}
        secret:
          crt: tls.crt
          key: tls.key
          name: ""
        type: letsencrypt
      labels: {}
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
        enabled: false
        ingress: []
      nodeSelector:
        jupyterhub-pool-name: core-pool
      pdb:
        enabled: true
        minAvailable: 1
      secretSync:
        image:
          name: jupyterhub/k8s-secret-sync
          tag: 0.9.1
        resources: {}
      secretToken: 15ae25fc67559b586199b6f1c6be9ee8629b4891280c867d58602def3ae69b6d
      service:
        annotations: {}
        labels: {}
        loadBalancerSourceRanges: []
        nodePorts: {}
        type: LoadBalancer
      traefik:
        hsts:
          includeSubdomains: false
          maxAge: 15724800
        image:
          name: traefik
          tag: v2.1
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 0.001
            memory: 256Mi
    rbac:
      enabled: true
    scheduling:
      corePods:
        nodeAffinity:
          matchNodePurpose: prefer
      podPriority:
        defaultPriority: 0
        enabled: true
        globalDefault: false
        userPlaceholderPriority: -10
      userPlaceholder:
        enabled: false
        replicas: 100
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      userScheduler:
        enabled: false
        image:
          name: gcr.io/google_containers/kube-scheduler-amd64
          tag: v1.13.12
        logLevel: 4
        nodeSelector:
          jupyterhub-pool-name: core-pool
        pdb:
          enabled: true
          minAvailable: 1
        policy: {}
        replicas: 2
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 0.01
            memory: 512Mi
    singleuser:
      cloudMetadata:
        enabled: false
        ip: 169.254.169.254
      cmd: jupyterhub-singleuser
      cpu: {}
      defaultUrl: /lab
      events: true
      extraAnnotations: {}
      extraContainers: []
      extraEnv:
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: '{JUPYTER_IMAGE_SPEC}'
      extraLabels:
        hub.jupyter.org/network-access-hub: "true"
      extraNodeAffinity:
        preferred: []
        required: []
      extraPodAffinity:
        preferred: []
        required: []
      extraPodAntiAffinity:
        preferred: []
        required: []
      extraPodConfig: {}
      extraResource:
        guarantees: {}
        limits: {}
      extraTolerations: []
      fsGid: 100
      image:
        name: 677861182063.dkr.ecr.us-east-2.amazonaws.com/2i2c-hub/user-image
        pullPolicy: IfNotPresent
        tag: 4157727a
      imagePullSecret:
        enabled: false
      initContainers:
      - command:
        - sh
        - -c
        - id && chown 1000:1000 /home/jovyan && ls -lhd /home/jovyan
        image: busybox
        name: volume-mount-hack
        securityContext:
          runAsUser: 0
        volumeMounts:
        - mountPath: /home/jovyan
          name: home
          subPath: homes/farallon/{username}
      lifecycleHooks: {}
      memory:
        guarantee: 1G
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
              except:
              - 169.254.169.254/32
        enabled: false
        ingress: []
      networkTools:
        image:
          name: jupyterhub/k8s-network-tools
          tag: 0.9.1
      nodeSelector: {}
      profileList:
      - description: ~4CPUs & ~15GB RAM
        display_name: 'Default: m5.xlarge'
        kubespawner_override:
          cpu_guarantee: 3
          mem_guarantee: 14G
          node_selector:
            jupyterhub-pool-name: user-pool-m5-xlarge
      - description: ~8CPUs & ~31GB RAM
        display_name: 'Default: m5.2xlarge'
        kubespawner_override:
          cpu_guarantee: 7
          mem_guarantee: 30G
          node_selector:
            jupyterhub-pool-name: user-pool-m5-2xlarge
      startTimeout: 300
      storage:
        capacity: 10Gi
        dynamic:
          pvcNameTemplate: claim-{username}{servername}
          storageAccessModes:
          - ReadWriteOnce
          volumeNameTemplate: volume-{username}{servername}
        extraLabels: {}
        extraVolumeMounts: []
        extraVolumes: []
        homeMountPath: /home/jovyan
        static:
          pvcName: home-nfs
          subPath: homes/farallon/{username}
        type: static
      uid: 1000
      workingDir: /home/jovyan
  rbac:
    enabled: true
nfsPVC:
  enabled: true
  nfs:
    serverIP: fs-7b129903.efs.us-east-2.amazonaws.com
    shareName: ""

HOOKS:
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/scheduling/priorityclass.yaml
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: farallon-staging-default-priority
  labels:
    component: default-priority
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
  annotations:
    # PriorityClasses must be added before the other resources reference them.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "-100"
value: 0
globalDefault: false
description: "A default priority higher than user placeholders priority."
MANIFEST:
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: farallon-staging
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: farallon-staging
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/scheduling/user-scheduler/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: farallon-staging
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/controller/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: controller-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/traefik/rbac.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: traefik-farallon-staging-dask-gateway
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/autohttps/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: autohttps
  labels:
    component: autohttps
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/secret.yaml
kind: Secret
apiVersion: v1
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
type: Opaque
data:
  jupyterhub-api-token: "OTU1NzZlYmQxMjQ4NjczNzdhNWI1NzZiYzhjYWQ0ZWFlMDlmYWIwOWMzNWZmYTg2ZGRhN2RmM2JhZTIzNWM1MA=="
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/secret.yaml
kind: Secret
apiVersion: v1
metadata:
  name: hub-secret
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
type: Opaque
data:
  proxy.token: "MTVhZTI1ZmM2NzU1OWI1ODYxOTliNmYxYzZiZTllZTg2MjliNDg5MTI4MGM4NjdkNTg2MDJkZWYzYWU2OWI2ZA=="
  auth.state.crypto-key: "YzY2OTUyNDAzNDU2M2FmM2RlYjkwMDFiMzU5MzM2ZjIwZTVjMmU5NmU0ZWVmYmZhMjU3ZTQyOTUwYjQxNDg2Yg=="
  values.yaml: "YXV0aDoKICBkdW1teToKICAgIHBhc3N3b3JkOiB3YXR3YXR3YXQKICBnaXRodWI6CiAgICBjbGllbnRTZWNyZXQ6IGJmODQ0NzliOTc3ZTlkMmNmNjkwOTdkMTc1NzQzY2UxYWZiMmU3MzEKaHViOgogIHNlcnZpY2VzOgogICAgZGFzay1nYXRld2F5OgogICAgICBhcGlUb2tlbjogOTU1NzZlYmQxMjQ4NjczNzdhNWI1NzZiYzhjYWQ0ZWFlMDlmYWIwOWMzNWZmYTg2ZGRhN2RmM2JhZTIzNWM1MA=="
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/controller/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: controller-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
data:
  dask_gateway_config.py: |-
    # Configure addresses
    c.KubeController.address = ":8000"
    c.KubeController.api_url = 'http://api-farallon-staging-dask-gateway.farallon-staging:8000/api'
    c.KubeController.gateway_instance = 'farallon-staging-dask-gateway'
    c.KubeController.proxy_prefix = "/services/dask-gateway"
    c.KubeController.proxy_web_middlewares = [
      {"name": 'clusters-prefix-farallon-staging-dask-gateway',
      "namespace": 'farallon-staging'}
    ]
    c.KubeController.log_level = "INFO"
    c.KubeController.completed_cluster_max_age = 86400
    c.KubeController.completed_cluster_cleanup_period = 600
    c.KubeController.backoff_base_delay = 0.1
    c.KubeController.backoff_max_delay = 300
    c.KubeController.k8s_api_rate_limit = 50
    c.KubeController.k8s_api_rate_limit_burst = 100
    c.KubeController.proxy_tcp_entrypoint = "web"
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
data:
  dask_gateway_config.py: |-
    
    
    import os
    import json

    _PROPERTIES = json.loads("{\"gateway\":{\"annotations\":{},\"auth\":{\"custom\":{\"options\":{}},\"jupyterhub\":{},\"kerberos\":{},\"simple\":{},\"type\":\"jupyterhub\"},\"backend\":{\"environment\":null,\"image\":{\"name\":\"daskgateway/dask-gateway\",\"pullPolicy\":\"IfNotPresent\",\"tag\":\"0.8.0\"},\"namespace\":null,\"scheduler\":{\"cores\":{\"limit\":1,\"request\":0.01},\"extraContainerConfig\":{},\"extraPodConfig\":{\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"k8s.dask.org/dedicated\",\"operator\":\"Equal\",\"value\":\"scheduler\"},{\"effect\":\"NoSchedule\",\"key\":\"k8s.dask.org_dedicated\",\"operator\":\"Equal\",\"value\":\"scheduler\"}]},\"memory\":{\"limit\":\"1G\",\"request\":\"128M\"}},\"worker\":{\"cores\":{},\"extraContainerConfig\":{\"securityContext\":{\"runAsGroup\":1000,\"runAsUser\":1000}},\"extraPodConfig\":{\"nodeSelector\":{\"dask-pool-name\":\"worker-2xlarge\"},\"securityContext\":{\"fsGroup\":1000},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"dask-dedicated\",\"operator\":\"Equal\",\"value\":\"worker\"}]},\"memory\":{}}},\"image\":{\"name\":\"daskgateway/dask-gateway-server\",\"pullPolicy\":\"IfNotPresent\",\"tag\":\"0.8.0\"},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"timeoutSeconds\":2},\"loglevel\":\"INFO\",\"prefix\":\"/services/dask-gateway\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"timeoutSeconds\":2},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{}}}}")

    def get_property(key, default=None):
        """Read a property from the configured helm values."""
        value = _PROPERTIES
        for key2 in key.split("."):
            if not isinstance(value, dict) or key2 not in value:
                return default
            value = value[key2]
        return value

    c.DaskGateway.log_level = "INFO"

    # Configure addresses
    c.DaskGateway.address = ":8000"
    c.KubeBackend.api_url = 'http://api-farallon-staging-dask-gateway.farallon-staging:8000/api'

    # Configure the backend
    c.DaskGateway.backend_class = "dask_gateway_server.backends.kubernetes.KubeBackend"
    c.KubeBackend.gateway_instance = "farallon-staging-dask-gateway"

    # Configure the dask cluster image
    image_name = get_property("gateway.backend.image.name")
    if image_name:
        image_tag = get_property("gateway.backend.image.tag")
        c.KubeClusterConfig.image = (
            "%s:%s" % (image_name, image_tag) if image_tag else image_name
        )

    # Forward dask cluster configuration
    for field, prop_name in [
        # Scheduler config
        ("scheduler_cores", "scheduler.cores.request"),
        ("scheduler_cores_limit", "scheduler.cores.limit"),
        ("scheduler_memory", "scheduler.memory.request"),
        ("scheduler_memory_limit", "scheduler.memory.limit"),
        ("scheduler_extra_container_config", "scheduler.extraContainerConfig"),
        ("scheduler_extra_pod_config", "scheduler.extraPodConfig"),
        # Worker config
        ("worker_cores", "worker.cores.request"),
        ("worker_cores_limit", "worker.cores.limit"),
        ("worker_memory", "worker.memory.request"),
        ("worker_memory_limit", "worker.memory.limit"),
        ("worker_extra_container_config", "worker.extraContainerConfig"),
        ("worker_extra_pod_config", "worker.extraPodConfig"),
        # Additional fields
        ("image_pull_policy", "image.pullPolicy"),
        ("environment", "environment"),
        ("namespace", "namespace"),
    ]:
        value = get_property("gateway.backend." + prop_name)
        if value is not None:
            setattr(c.KubeClusterConfig, field, value)

    # Authentication
    auth_type = get_property("gateway.auth.type")
    if auth_type == "simple":
        c.DaskGateway.authenticator_class = "dask_gateway_server.auth.SimpleAuthenticator"
        password = get_property("gateway.auth.simple.password")
        if password is not None:
            c.SimpleAuthenticator.password = password
    elif auth_type == "kerberos":
        c.DaskGateway.authenticator_class = "dask_gateway_server.auth.KerberosAuthenticator"
        keytab = get_property("gateway.auth.kerberos.keytab")
        if keytab is not None:
            c.KerberosAuthenticator.keytab = keytab
    elif auth_type == "jupyterhub":
        c.DaskGateway.authenticator_class = "dask_gateway_server.auth.JupyterHubAuthenticator"
        api_url = get_property("gateway.auth.jupyterhub.apiUrl")
        if api_url is None:
            try:
                api_url = "http://{HUB_SERVICE_HOST}:{HUB_SERVICE_PORT}/hub/api".format(**os.environ)
            except Exception:
                raise ValueError(
                    "Failed to infer JupyterHub API url from environment, "
                    "please specify `gateway.auth.jupyterhub.apiUrl` in "
                    "your config file"
                )
        c.DaskGateway.JupyterHubAuthenticator.jupyterhub_api_url = api_url
    elif auth_type == "custom":
        auth_cls = get_property("gateway.auth.custom.class")
        c.DaskGateway.authenticator_class = auth_cls
        auth_cls_name = auth_cls.rsplit('.', 1)[-1]
        auth_config = c[auth_cls_name]
        auth_config.update(get_property("gateway.auth.custom.config") or {})
    else:
        raise ValueError("Unknown authenticator type %r" % auth_type)
    
    # From gateway.extraConfig.idle
    # timeout after 30 minutes of inactivity
    c.KubeClusterConfig.idle_timeout = 1800
    
    # From gateway.extraConfig.optionHandler
    from dask_gateway_server.options import Options, Integer, Float, String
    def cluster_options(user):
       def option_handler(options):
           if ":" not in options.image:
               raise ValueError("When specifying an image you must also provide a tag")
           extra_annotations = {
               "hub.jupyter.org/username": user.name,
               "prometheus.io/scrape": "true",
               "prometheus.io/port": "8787",
           }
           extra_labels = {
               "hub.jupyter.org/username": user.name,
           }
           return {
               "worker_cores_limit": options.worker_cores,
               "worker_cores": min(options.worker_cores / 2, 1),
               "worker_memory": "%fG" % options.worker_memory,
               "image": options.image,
               "scheduler_extra_pod_annotations": extra_annotations,
               "worker_extra_pod_annotations": extra_annotations,
               "scheduler_extra_pod_labels": extra_labels,
               "worker_extra_pod_labels": extra_labels,
           }
       return Options(
           Integer("worker_cores", 2, min=1, max=16, label="Worker Cores"),
           Float("worker_memory", 4, min=1, max=32, label="Worker Memory (GiB)"),
           String("image", default="pangeo/pangeo-notebook:latest", label="Image"),
           handler=option_handler,
       )
    c.Backend.cluster_options = cluster_options
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: hub-config
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
data:
  values.yaml: |
    Chart:
      Name: jupyterhub
      Version: 0.9.1
    Release:
      Name: farallon-staging
      Namespace: farallon-staging
      Service: Helm
    auth:
      admin:
        access: true
      dummy: {}
      github:
        callbackUrl: https://staging.farallon.2i2c.cloud/hub/oauth_callback
        clientId: 9c4b51ba2018557f58a3
      ldap:
        dn:
          search: {}
          user: {}
        user: {}
      state:
        enabled: true
      type: github
      whitelist: {}
    cull:
      concurrency: 1
      enabled: true
      every: 1800
      maxAge: 0
      removeNamedServers: false
      timeout: 3600
      users: false
    custom: {}
    debug:
      enabled: false
    hub:
      allowNamedServers: true
      annotations: {}
      baseUrl: /
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      db:
        pvc:
          accessModes:
          - ReadWriteOnce
          annotations: {}
          selector: {}
          storage: 1Gi
        type: sqlite-pvc
      deploymentStrategy:
        type: Recreate
      extraConfig:
        00-add-dask-gateway-values: |
          # 1. Sets `DASK_GATEWAY__PROXY_ADDRESS` in the singleuser environment.
          # 2. Adds the URL for the Dask Gateway JupyterHub service.
          import os
    
          # These are set by jupyterhub.
          release_name = os.environ["HELM_RELEASE_NAME"]
          release_namespace = os.environ["POD_NAMESPACE"]
    
          if "PROXY_HTTP_SERVICE_HOST" in os.environ:
              # https is enabled, we want to use the internal http service.
              gateway_address = "http://{}:{}/services/dask-gateway/".format(
                  os.environ["PROXY_HTTP_SERVICE_HOST"],
                  os.environ["PROXY_HTTP_SERVICE_PORT"],
              )
              print("Setting DASK_GATEWAY__ADDRESS {} from HTTP service".format(gateway_address))
          else:
              gateway_address = "http://proxy-public/services/dask-gateway"
              print("Setting DASK_GATEWAY__ADDRESS {}".format(gateway_address))
    
          # Internal address to connect to the Dask Gateway.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__ADDRESS", gateway_address)
          # Internal address for the Dask Gateway proxy.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__PROXY_ADDRESS", "gateway://traefik-{}-dask-gateway.{}:80".format(release_name, release_namespace))
          # Relative address for the dashboard link.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__PUBLIC_ADDRESS", "/services/dask-gateway/")
          # Use JupyterHub to authenticate with Dask Gateway.
          c.KubeSpawner.environment.setdefault("DASK_GATEWAY__AUTH__TYPE", "jupyterhub")
    
          # Adds Dask Gateway as a JupyterHub service to make the gateway available at
          # {HUB_URL}/services/dask-gateway
          service_url = "http://traefik-{}-dask-gateway.{}".format(release_name, release_namespace)
          for service in c.JupyterHub.services:
              if service["name"] == "dask-gateway":
                  if not service.get("url", None):
                      print("Adding dask-gateway service URL")
                      service.setdefault("url", service_url)
                  break
          else:
              print("dask-gateway service not found. Did you set jupyterhub.hub.services.dask-gateway.apiToken?")
        01-working-dir: |
          # Make sure working directory is ${HOME}
          # hubploy has a bug where it unconditionally puts workingdir to be /srv/repo
          c.KubeSpawner.working_dir = '/home/jovyan'
        02-prometheus: |
          # Allow unauthenticated prometheus requests
          # Otherwise our prometheus server can't get to these
          c.JupyterHub.authenticate_prometheus = False
        03-no-setuid: |
          # Disable 'sudo' & similar binaries, regardless of image contents
          c.KubeSpawner.extra_container_config = {
            'securityContext': {
              # Explicitly disallow setuid binaries from working inside the container
              'allowPrivilegeEscalation': False
            }
          }
      extraContainers:
      - args:
        - -c
        - while true; do git fetch origin; git reset --hard origin/master; sleep 5m; done
        command:
        - /bin/sh
        image: alpine/git
        name: templates-sync
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /srv/repo
          name: custom-templates
        workingDir: /srv/repo
      extraVolumeMounts:
      - mountPath: /usr/local/share/jupyterhub/custom_templates
        name: custom-templates
        subPath: templates
      - mountPath: /usr/local/share/jupyterhub/static/extra-assets
        name: custom-templates
        subPath: extra-assets
      extraVolumes:
      - emptyDir: {}
        name: custom-templates
      fsGid: 1000
      image:
        name: jupyterhub/k8s-hub
        tag: 0.9.1
      imagePullSecret:
        enabled: false
      initContainers:
      - args:
        - clone
        - --depth=1
        - --single-branch
        - --
        - https://github.com/2i2c-utoronto/homepage
        - /srv/repo
        image: alpine/git
        name: templates-clone
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /srv/repo
          name: custom-templates
      labels: {}
      livenessProbe:
        enabled: false
        initialDelaySeconds: 30
        periodSeconds: 10
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
        enabled: false
        ingress: []
      nodeSelector:
        jupyterhub-pool-name: core-pool
      pdb:
        enabled: true
        minAvailable: 1
      readinessProbe:
        enabled: false
        initialDelaySeconds: 0
        periodSeconds: 10
      resources:
        limits:
          memory: 2Gi
        requests:
          cpu: 0.001
          memory: 512Mi
      service:
        annotations: {}
        ports: {}
        type: ClusterIP
      services:
        dask-gateway: {}
      templatePaths: []
      templateVars: {}
      uid: 1000
    scheduling:
      corePods:
        nodeAffinity:
          matchNodePurpose: prefer
      podPriority:
        defaultPriority: 0
        enabled: true
        globalDefault: false
        userPlaceholderPriority: -10
      userPlaceholder:
        enabled: false
        replicas: 100
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      userScheduler:
        enabled: false
        image:
          name: gcr.io/google_containers/kube-scheduler-amd64
          tag: v1.13.12
        logLevel: 4
        nodeSelector:
          jupyterhub-pool-name: core-pool
        pdb:
          enabled: true
          minAvailable: 1
        policy: {}
        replicas: 2
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 0.01
            memory: 512Mi
    singleuser:
      cloudMetadata:
        enabled: false
        ip: 169.254.169.254
      cmd: jupyterhub-singleuser
      cpu: {}
      defaultUrl: /lab
      events: true
      extraAnnotations: {}
      extraContainers: []
      extraEnv:
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: '{JUPYTER_IMAGE_SPEC}'
      extraLabels:
        hub.jupyter.org/network-access-hub: "true"
      extraNodeAffinity:
        preferred: []
        required: []
      extraPodAffinity:
        preferred: []
        required: []
      extraPodAntiAffinity:
        preferred: []
        required: []
      extraPodConfig: {}
      extraResource:
        guarantees: {}
        limits: {}
      extraTolerations: []
      fsGid: 100
      image:
        name: 677861182063.dkr.ecr.us-east-2.amazonaws.com/2i2c-hub/user-image
        pullPolicy: IfNotPresent
        tag: 4157727a
      imagePullSecret:
        enabled: false
      initContainers:
      - command:
        - sh
        - -c
        - id && chown 1000:1000 /home/jovyan && ls -lhd /home/jovyan
        image: busybox
        name: volume-mount-hack
        securityContext:
          runAsUser: 0
        volumeMounts:
        - mountPath: /home/jovyan
          name: home
          subPath: homes/farallon/{username}
      lifecycleHooks: {}
      memory:
        guarantee: 1G
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
              except:
              - 169.254.169.254/32
        enabled: false
        ingress: []
      networkTools:
        image:
          name: jupyterhub/k8s-network-tools
          tag: 0.9.1
      nodeSelector: {}
      profileList:
      - description: ~4CPUs & ~15GB RAM
        display_name: 'Default: m5.xlarge'
        kubespawner_override:
          cpu_guarantee: 3
          mem_guarantee: 14G
          node_selector:
            jupyterhub-pool-name: user-pool-m5-xlarge
      - description: ~8CPUs & ~31GB RAM
        display_name: 'Default: m5.2xlarge'
        kubespawner_override:
          cpu_guarantee: 7
          mem_guarantee: 30G
          node_selector:
            jupyterhub-pool-name: user-pool-m5-2xlarge
      startTimeout: 300
      storage:
        capacity: 10Gi
        dynamic:
          pvcNameTemplate: claim-{username}{servername}
          storageAccessModes:
          - ReadWriteOnce
          volumeNameTemplate: volume-{username}{servername}
        extraLabels: {}
        extraVolumeMounts: []
        extraVolumes: []
        homeMountPath: /home/jovyan
        static:
          pvcName: home-nfs
          subPath: homes/farallon/{username}
        type: static
      uid: 1000
      workingDir: /home/jovyan
  cull_idle_servers.py: |
    #!/usr/bin/env python3
    # Imported from https://github.com/jupyterhub/jupyterhub/blob/6b1046697/examples/cull-idle/cull_idle_servers.py
    """script to monitor and cull idle single-user servers
  
    Caveats:
  
    last_activity is not updated with high frequency,
    so cull timeout should be greater than the sum of:
  
    - single-user websocket ping interval (default: 30s)
    - JupyterHub.last_activity_interval (default: 5 minutes)
  
    You can run this as a service managed by JupyterHub with this in your config::
  
  
        c.JupyterHub.services = [
            {
                'name': 'cull-idle',
                'admin': True,
                'command': 'python cull_idle_servers.py --timeout=3600'.split(),
            }
        ]
  
    Or run it manually by generating an API token and storing it in `JUPYTERHUB_API_TOKEN`:
  
        export JUPYTERHUB_API_TOKEN=`jupyterhub token`
        python cull_idle_servers.py [--timeout=900] [--url=http://127.0.0.1:8081/hub/api]
    """
  
    from datetime import datetime, timezone
    from functools import partial
    import json
    import os
  
    try:
        from urllib.parse import quote
    except ImportError:
        from urllib import quote
  
    import dateutil.parser
  
    from tornado.gen import coroutine, multi
    from tornado.locks import Semaphore
    from tornado.log import app_log
    from tornado.httpclient import AsyncHTTPClient, HTTPRequest
    from tornado.ioloop import IOLoop, PeriodicCallback
    from tornado.options import define, options, parse_command_line
  
  
    def parse_date(date_string):
        """Parse a timestamp
  
        If it doesn't have a timezone, assume utc
  
        Returned datetime object will always be timezone-aware
        """
        dt = dateutil.parser.parse(date_string)
        if not dt.tzinfo:
            # assume naïve timestamps are UTC
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
  
  
    def format_td(td):
        """
        Nicely format a timedelta object
  
        as HH:MM:SS
        """
        if td is None:
            return "unknown"
        if isinstance(td, str):
            return td
        seconds = int(td.total_seconds())
        h = seconds // 3600
        seconds = seconds % 3600
        m = seconds // 60
        seconds = seconds % 60
        return f"{h:02}:{m:02}:{seconds:02}"
  
  
    @coroutine
    def cull_idle(
        url,
        api_token,
        inactive_limit,
        cull_users=False,
        remove_named_servers=False,
        max_age=0,
        concurrency=10,
    ):
        """Shutdown idle single-user servers
  
        If cull_users, inactive *users* will be deleted as well.
        """
        auth_header = {
            'Authorization': 'token %s' % api_token,
        }
        req = HTTPRequest(
            url=url + '/users',
            headers=auth_header,
        )
        now = datetime.now(timezone.utc)
        client = AsyncHTTPClient()
  
        if concurrency:
            semaphore = Semaphore(concurrency)
            @coroutine
            def fetch(req):
                """client.fetch wrapped in a semaphore to limit concurrency"""
                yield semaphore.acquire()
                try:
                    return (yield client.fetch(req))
                finally:
                    yield semaphore.release()
        else:
            fetch = client.fetch
  
        resp = yield fetch(req)
        users = json.loads(resp.body.decode('utf8', 'replace'))
        futures = []
  
        @coroutine
        def handle_server(user, server_name, server):
            """Handle (maybe) culling a single server
  
            Returns True if server is now stopped (user removable),
            False otherwise.
            """
            log_name = user['name']
            if server_name:
                log_name = '%s/%s' % (user['name'], server_name)
            if server.get('pending'):
                app_log.warning(
                    "Not culling server %s with pending %s",
                    log_name, server['pending'])
                return False
  
            if server.get('started'):
                age = now - parse_date(server['started'])
            else:
                # started may be undefined on jupyterhub < 0.9
                age = None
  
            # check last activity
            # last_activity can be None in 0.9
            if server['last_activity']:
                inactive = now - parse_date(server['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'started' field which is never None
                # for running servers
                inactive = age
  
            should_cull = (inactive is not None and
                           inactive.total_seconds() >= inactive_limit)
            if should_cull:
                app_log.info(
                    "Culling server %s (inactive for %s)",
                    log_name, format_td(inactive))
  
            if max_age and not should_cull:
                # only check started if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling server %s (age: %s, inactive for %s)",
                        log_name, format_td(age), format_td(inactive))
                    should_cull = True
  
            if not should_cull:
                app_log.debug(
                    "Not culling server %s (age: %s, inactive for %s)",
                    log_name, format_td(age), format_td(inactive))
                return False
  
            body = None
            if server_name:
                # culling a named server
                # A named server can be stopped and kept available to the user
                # for starting again or stopped and removed. To remove the named
                # server we have to pass an additional option in the body of our
                # DELETE request.
                delete_url = url + "/users/%s/servers/%s" % (
                    quote(user['name']),
                    quote(server['name']),
                )
                if remove_named_servers:
                    body = json.dumps({"remove": True})
            else:
                delete_url = url + '/users/%s/server' % quote(user['name'])
  
            req = HTTPRequest(
                url=delete_url,
                method='DELETE',
                headers=auth_header,
                body=body,
                allow_nonstandard_methods=True,
            )
            resp = yield fetch(req)
            if resp.code == 202:
                app_log.warning(
                    "Server %s is slow to stop",
                    log_name,
                )
                # return False to prevent culling user with pending shutdowns
                return False
            return True
  
        @coroutine
        def handle_user(user):
            """Handle one user.
  
            Create a list of their servers, and async exec them.  Wait for
            that to be done, and if all servers are stopped, possibly cull
            the user.
            """
            # shutdown servers first.
            # Hub doesn't allow deleting users with running servers.
            # named servers contain the 'servers' dict
            if 'servers' in user:
                servers = user['servers']
            # Otherwise, server data is intermingled in with the user
            # model
            else:
                servers = {}
                if user['server']:
                    servers[''] = {
                        'started': user.get('started'),
                        'last_activity': user['last_activity'],
                        'pending': user['pending'],
                        'url': user['server'],
                    }
            server_futures = [
                handle_server(user, server_name, server)
                for server_name, server in servers.items()
            ]
            results = yield multi(server_futures)
            if not cull_users:
                return
            # some servers are still running, cannot cull users
            still_alive = len(results) - sum(results)
            if still_alive:
                app_log.debug(
                    "Not culling user %s with %i servers still alive",
                    user['name'], still_alive)
                return False
  
            should_cull = False
            if user.get('created'):
                age = now - parse_date(user['created'])
            else:
                # created may be undefined on jupyterhub < 0.9
                age = None
  
            # check last activity
            # last_activity can be None in 0.9
            if user['last_activity']:
                inactive = now - parse_date(user['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'created' field which is never None
                inactive = age
  
            should_cull = (inactive is not None and
                           inactive.total_seconds() >= inactive_limit)
            if should_cull:
                app_log.info(
                    "Culling user %s (inactive for %s)",
                    user['name'], inactive)
  
            if max_age and not should_cull:
                # only check created if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling user %s (age: %s, inactive for %s)",
                        user['name'], format_td(age), format_td(inactive))
                    should_cull = True
  
            if not should_cull:
                app_log.debug(
                    "Not culling user %s (created: %s, last active: %s)",
                    user['name'], format_td(age), format_td(inactive))
                return False
  
            req = HTTPRequest(
                url=url + '/users/%s' % user['name'],
                method='DELETE',
                headers=auth_header,
            )
            yield fetch(req)
            return True
  
        for user in users:
            futures.append((user['name'], handle_user(user)))
  
        for (name, f) in futures:
            try:
                result = yield f
            except Exception:
                app_log.exception("Error processing %s", name)
            else:
                if result:
                    app_log.debug("Finished culling %s", name)
  
  
    if __name__ == '__main__':
        define(
            'url',
            default=os.environ.get('JUPYTERHUB_API_URL'),
            help="The JupyterHub API URL",
        )
        define('timeout', default=600, help="The idle timeout (in seconds)")
        define('cull_every', default=0,
               help="The interval (in seconds) for checking for idle servers to cull")
        define('max_age', default=0,
               help="The maximum age (in seconds) of servers that should be culled even if they are active")
        define('cull_users', default=False,
               help="""Cull users in addition to servers.
                    This is for use in temporary-user cases such as BinderHub.""",
               )
        define('remove_named_servers', default=False,
               help="""Remove named servers in addition to stopping them.
                    This is useful for a BinderHub that uses authentication and named servers.""",
               )
        define('concurrency', default=10,
               help="""Limit the number of concurrent requests made to the Hub.
  
                    Deleting a lot of users at the same time can slow down the Hub,
                    so limit the number of API requests we have outstanding at any given time.
                    """
               )
  
        parse_command_line()
        if not options.cull_every:
            options.cull_every = options.timeout // 2
        api_token = os.environ['JUPYTERHUB_API_TOKEN']
  
        try:
            AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
        except ImportError as e:
            app_log.warning(
                "Could not load pycurl: %s\n"
                "pycurl is recommended if you have a large number of users.",
                e)
  
        loop = IOLoop.current()
        cull = partial(
            cull_idle,
            url=options.url,
            api_token=api_token,
            inactive_limit=options.timeout,
            cull_users=options.cull_users,
            remove_named_servers=options.remove_named_servers,
            max_age=options.max_age,
            concurrency=options.concurrency,
        )
        # schedule first cull immediately
        # because PeriodicCallback doesn't start until the end of the first interval
        loop.add_callback(cull)
        # schedule periodic cull
        pc = PeriodicCallback(cull, 1e3 * options.cull_every)
        pc.start()
        try:
            loop.start()
        except KeyboardInterrupt:
            pass
  jupyterhub_config.py: "import os\nimport re\nimport sys\n\nfrom tornado.httpclient
    import AsyncHTTPClient\nfrom kubernetes import client\nfrom jupyterhub.utils import
    url_path_join\n\n# Make sure that modules placed in the same directory as the jupyterhub
    config are added to the pythonpath\nconfiguration_directory = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0,
    configuration_directory)\n\nfrom z2jh import get_config, set_config_if_not_none\n\n#
    Configure JupyterHub to use the curl backend for making HTTP requests,\n# rather
    than the pure-python implementations. The default one starts\n# being too slow to
    make a large number of requests to the proxy API\n# at the rate required.\nAsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")\n\n#
    Patch for CVE-2020-15110: change default template for named servers\n# with kubespawner
    0.11, {servername} *contains* a leading '-'\n# leading `{user}-{server}` to create
    `username--servername`,\n# preventing collision.\n# kubespawner 0.12 does not contain
    `-` in {servername},\n# and uses default name template `{username}--{servername}`\n#
    so this patch must not be used with kubespawner >= 0.12\n\nfrom distutils.version
    import LooseVersion as V\nfrom traitlets import default\nimport kubespawner\nfrom
    kubespawner import KubeSpawner\n\n\nclass PatchedKubeSpawner(KubeSpawner):\n    @default(\"pod_name_template\")\n
    \   def _default_pod_name_template(self):\n        if self.name:\n            return
    \"jupyter-{username}-{servername}\"\n        else:\n            return \"jupyter-{username}\"\n\n
    \   @default(\"pvc_name_template\")\n    def _default_pvc_name_template(self):\n
    \       if self.name:\n            return \"claim-{username}-{servername}\"\n        else:\n
    \           return \"claim-{username}\"\n\n\nkubespawner_version = getattr(kubespawner,
    \"__version__\", \"0.11\")\nif V(kubespawner_version) < V(\"0.11.999\"):\n    c.JupyterHub.spawner_class
    = PatchedKubeSpawner\nelse:\n    # 0.12 or greater, defaults are safe\n    c.JupyterHub.spawner_class
    = KubeSpawner\n\n# Connect to a proxy running in a different pod\nc.ConfigurableHTTPProxy.api_url
    = 'http://{}:{}'.format(os.environ['PROXY_API_SERVICE_HOST'], int(os.environ['PROXY_API_SERVICE_PORT']))\nc.ConfigurableHTTPProxy.should_start
    = False\n\n# Do not shut down user pods when hub is restarted\nc.JupyterHub.cleanup_servers
    = False\n\n# Check that the proxy has routes appropriately setup\nc.JupyterHub.last_activity_interval
    = 60\n\n# Don't wait at all before redirecting a spawning user to the progress page\nc.JupyterHub.tornado_settings
    = {\n    'slow_spawn_timeout': 0,\n}\n\n\ndef camelCaseify(s):\n    \"\"\"convert
    snake_case to camelCase\n\n    For the common case where some_value is set from
    someValue\n    so we don't have to specify the name twice.\n    \"\"\"\n    return
    re.sub(r\"_([a-z])\", lambda m: m.group(1).upper(), s)\n\n\n# configure the hub
    db connection\ndb_type = get_config('hub.db.type')\nif db_type == 'sqlite-pvc':\n
    \   c.JupyterHub.db_url = \"sqlite:///jupyterhub.sqlite\"\nelif db_type == \"sqlite-memory\":\n
    \   c.JupyterHub.db_url = \"sqlite://\"\nelse:\n    set_config_if_not_none(c.JupyterHub,
    \"db_url\", \"hub.db.url\")\n    \n\nfor trait, cfg_key in (\n    # Max number of
    servers that can be spawning at any one time\n    ('concurrent_spawn_limit', None),\n
    \   # Max number of servers to be running at one time\n    ('active_server_limit',
    None),\n    # base url prefix\n    ('base_url', None),\n    ('allow_named_servers',
    None),\n    ('named_server_limit_per_user', None),\n    ('authenticate_prometheus',
    None),\n    ('redirect_to_server', None),\n    ('shutdown_on_logout', None),\n    ('template_paths',
    None),\n    ('template_vars', None),\n):\n    if cfg_key is None:\n        cfg_key
    = camelCaseify(trait)\n    set_config_if_not_none(c.JupyterHub, trait, 'hub.' +
    cfg_key)\n\nc.JupyterHub.ip = os.environ['PROXY_PUBLIC_SERVICE_HOST']\nc.JupyterHub.port
    = int(os.environ['PROXY_PUBLIC_SERVICE_PORT'])\n\n# the hub should listen on all
    interfaces, so the proxy can access it\nc.JupyterHub.hub_ip = '0.0.0.0'\n\n# implement
    common labels\n# this duplicates the jupyterhub.commonLabels helper\ncommon_labels
    = c.KubeSpawner.common_labels = {}\ncommon_labels['app'] = get_config(\n    \"nameOverride\",\n
    \   default=get_config(\"Chart.Name\", \"jupyterhub\"),\n)\ncommon_labels['heritage']
    = \"jupyterhub\"\nchart_name = get_config('Chart.Name')\nchart_version = get_config('Chart.Version')\nif
    chart_name and chart_version:\n    common_labels['chart'] = \"{}-{}\".format(\n
    \       chart_name, chart_version.replace('+', '_'),\n    )\nrelease = get_config('Release.Name')\nif
    release:\n    common_labels['release'] = release\n\nc.KubeSpawner.namespace = os.environ.get('POD_NAMESPACE',
    'default')\n\n# Max number of consecutive failures before the Hub restarts itself\n#
    requires jupyterhub 0.9.2\nset_config_if_not_none(\n    c.Spawner,\n    'consecutive_failure_limit',\n
    \   'hub.consecutiveFailureLimit',\n)\n\nfor trait, cfg_key in (\n    ('start_timeout',
    None),\n    ('image_pull_policy', 'image.pullPolicy'),\n    ('events_enabled', 'events'),\n
    \   ('extra_labels', None),\n    ('extra_annotations', None),\n    ('uid', None),\n
    \   ('fs_gid', None),\n    ('service_account', 'serviceAccountName'),\n    ('storage_extra_labels',
    'storage.extraLabels'),\n    ('tolerations', 'extraTolerations'),\n    ('node_selector',
    None),\n    ('node_affinity_required', 'extraNodeAffinity.required'),\n    ('node_affinity_preferred',
    'extraNodeAffinity.preferred'),\n    ('pod_affinity_required', 'extraPodAffinity.required'),\n
    \   ('pod_affinity_preferred', 'extraPodAffinity.preferred'),\n    ('pod_anti_affinity_required',
    'extraPodAntiAffinity.required'),\n    ('pod_anti_affinity_preferred', 'extraPodAntiAffinity.preferred'),\n
    \   ('lifecycle_hooks', None),\n    ('init_containers', None),\n    ('extra_containers',
    None),\n    ('mem_limit', 'memory.limit'),\n    ('mem_guarantee', 'memory.guarantee'),\n
    \   ('cpu_limit', 'cpu.limit'),\n    ('cpu_guarantee', 'cpu.guarantee'),\n    ('extra_resource_limits',
    'extraResource.limits'),\n    ('extra_resource_guarantees', 'extraResource.guarantees'),\n
    \   ('environment', 'extraEnv'),\n    ('profile_list', None),\n    ('extra_pod_config',
    None),\n):\n    if cfg_key is None:\n        cfg_key = camelCaseify(trait)\n    set_config_if_not_none(c.KubeSpawner,
    trait, 'singleuser.' + cfg_key)\n\nimage = get_config(\"singleuser.image.name\")\nif
    image:\n    tag = get_config(\"singleuser.image.tag\")\n    if tag:\n        image
    = \"{}:{}\".format(image, tag)\n\n    c.KubeSpawner.image = image\n\nif get_config('singleuser.imagePullSecret.enabled'):\n
    \   c.KubeSpawner.image_pull_secrets = 'singleuser-image-credentials'\n\n# scheduling:\nif
    get_config('scheduling.userScheduler.enabled'):\n    c.KubeSpawner.scheduler_name
    = os.environ['HELM_RELEASE_NAME'] + \"-user-scheduler\"\nif get_config('scheduling.podPriority.enabled'):\n
    \   c.KubeSpawner.priority_class_name = os.environ['HELM_RELEASE_NAME'] + \"-default-priority\"\n\n#
    add node-purpose affinity\nmatch_node_purpose = get_config('scheduling.userPods.nodeAffinity.matchNodePurpose')\nif
    match_node_purpose:\n    node_selector = dict(\n        matchExpressions=[\n            dict(\n
    \               key=\"hub.jupyter.org/node-purpose\",\n                operator=\"In\",\n
    \               values=[\"user\"],\n            )\n        ],\n    )\n    if match_node_purpose
    == 'prefer':\n        c.KubeSpawner.node_affinity_preferred.append(\n            dict(\n
    \               weight=100,\n                preference=node_selector,\n            ),\n
    \       )\n    elif match_node_purpose == 'require':\n        c.KubeSpawner.node_affinity_required.append(node_selector)\n
    \   elif match_node_purpose == 'ignore':\n        pass\n    else:\n        raise
    ValueError(\"Unrecognized value for matchNodePurpose: %r\" % match_node_purpose)\n\n#
    add dedicated-node toleration\nfor key in (\n    'hub.jupyter.org/dedicated',\n
    \   # workaround GKE not supporting / in initial node taints\n    'hub.jupyter.org_dedicated',\n):\n
    \   c.KubeSpawner.tolerations.append(\n        dict(\n            key=key,\n            operator='Equal',\n
    \           value='user',\n            effect='NoSchedule',\n        )\n    )\n\n#
    Configure dynamically provisioning pvc\nstorage_type = get_config('singleuser.storage.type')\n\nif
    storage_type == 'dynamic':\n    pvc_name_template = get_config('singleuser.storage.dynamic.pvcNameTemplate')\n
    \   c.KubeSpawner.pvc_name_template = pvc_name_template\n    volume_name_template
    = get_config('singleuser.storage.dynamic.volumeNameTemplate')\n    c.KubeSpawner.storage_pvc_ensure
    = True\n    set_config_if_not_none(c.KubeSpawner, 'storage_class', 'singleuser.storage.dynamic.storageClass')\n
    \   set_config_if_not_none(c.KubeSpawner, 'storage_access_modes', 'singleuser.storage.dynamic.storageAccessModes')\n
    \   set_config_if_not_none(c.KubeSpawner, 'storage_capacity', 'singleuser.storage.capacity')\n\n
    \   # Add volumes to singleuser pods\n    c.KubeSpawner.volumes = [\n        {\n
    \           'name': volume_name_template,\n            'persistentVolumeClaim':
    {\n                'claimName': pvc_name_template\n            }\n        }\n    ]\n
    \   c.KubeSpawner.volume_mounts = [\n        {\n            'mountPath': get_config('singleuser.storage.homeMountPath'),\n
    \           'name': volume_name_template\n        }\n    ]\nelif storage_type ==
    'static':\n    pvc_claim_name = get_config('singleuser.storage.static.pvcName')\n
    \   c.KubeSpawner.volumes = [{\n        'name': 'home',\n        'persistentVolumeClaim':
    {\n            'claimName': pvc_claim_name\n        }\n    }]\n\n    c.KubeSpawner.volume_mounts
    = [{\n        'mountPath': get_config('singleuser.storage.homeMountPath'),\n        'name':
    'home',\n        'subPath': get_config('singleuser.storage.static.subPath')\n    }]\n\nc.KubeSpawner.volumes.extend(get_config('singleuser.storage.extraVolumes',
    []))\nc.KubeSpawner.volume_mounts.extend(get_config('singleuser.storage.extraVolumeMounts',
    []))\n\n# Gives spawned containers access to the API of the hub\nc.JupyterHub.hub_connect_ip
    = os.environ['HUB_SERVICE_HOST']\nc.JupyterHub.hub_connect_port = int(os.environ['HUB_SERVICE_PORT'])\n\n#
    Allow switching authenticators easily\nauth_type = get_config('auth.type')\nemail_domain
    = 'local'\n\ncommon_oauth_traits = (\n        ('client_id', None),\n        ('client_secret',
    None),\n        ('oauth_callback_url', 'callbackUrl'),\n)\n\nif auth_type == 'google':\n
    \   c.JupyterHub.authenticator_class = 'oauthenticator.GoogleOAuthenticator'\n    for
    trait, cfg_key in common_oauth_traits + (\n        ('hosted_domain', None),\n        ('login_service',
    None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n
    \       set_config_if_not_none(c.GoogleOAuthenticator, trait, 'auth.google.' + cfg_key)\n
    \   email_domain = get_config('auth.google.hostedDomain')\nelif auth_type == 'github':\n
    \   c.JupyterHub.authenticator_class = 'oauthenticator.github.GitHubOAuthenticator'\n
    \   for trait, cfg_key in common_oauth_traits + (\n        ('github_organization_whitelist',
    'orgWhitelist'),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n
    \       set_config_if_not_none(c.GitHubOAuthenticator, trait, 'auth.github.' + cfg_key)\nelif
    auth_type == 'cilogon':\n    c.JupyterHub.authenticator_class = 'oauthenticator.CILogonOAuthenticator'\n
    \   for trait, cfg_key in common_oauth_traits:\n        if cfg_key is None:\n            cfg_key
    = camelCaseify(trait)\n        set_config_if_not_none(c.CILogonOAuthenticator, trait,
    'auth.cilogon.' + cfg_key)\nelif auth_type == 'gitlab':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.gitlab.GitLabOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('gitlab_group_whitelist', None),\n        ('gitlab_project_id_whitelist',
    None),\n        ('gitlab_url', None),\n    ):\n        if cfg_key is None:\n            cfg_key
    = camelCaseify(trait)\n        set_config_if_not_none(c.GitLabOAuthenticator, trait,
    'auth.gitlab.' + cfg_key)\nelif auth_type == 'azuread':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.azuread.AzureAdOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('tenant_id', None),\n        ('username_claim', None),\n    ):\n        if
    cfg_key is None:\n            cfg_key = camelCaseify(trait)\n\n        set_config_if_not_none(c.AzureAdOAuthenticator,
    trait, 'auth.azuread.' + cfg_key)\nelif auth_type == 'mediawiki':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.mediawiki.MWOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('index_url', None),\n    ):\n        if cfg_key is None:\n            cfg_key
    = camelCaseify(trait)\n        set_config_if_not_none(c.MWOAuthenticator, trait,
    'auth.mediawiki.' + cfg_key)\nelif auth_type == 'globus':\n    c.JupyterHub.authenticator_class
    = 'oauthenticator.globus.GlobusOAuthenticator'\n    for trait, cfg_key in common_oauth_traits
    + (\n        ('identity_provider', None),\n    ):\n        if cfg_key is None:\n
    \           cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.GlobusOAuthenticator,
    trait, 'auth.globus.' + cfg_key)\nelif auth_type == 'hmac':\n    c.JupyterHub.authenticator_class
    = 'hmacauthenticator.HMACAuthenticator'\n    c.HMACAuthenticator.secret_key = bytes.fromhex(get_config('auth.hmac.secretKey'))\nelif
    auth_type == 'dummy':\n    c.JupyterHub.authenticator_class = 'dummyauthenticator.DummyAuthenticator'\n
    \   set_config_if_not_none(c.DummyAuthenticator, 'password', 'auth.dummy.password')\nelif
    auth_type == 'tmp':\n    c.JupyterHub.authenticator_class = 'tmpauthenticator.TmpAuthenticator'\nelif
    auth_type == 'lti':\n    c.JupyterHub.authenticator_class = 'ltiauthenticator.LTIAuthenticator'\n
    \   set_config_if_not_none(c.LTIAuthenticator, 'consumers', 'auth.lti.consumers')\nelif
    auth_type == 'ldap':\n    c.JupyterHub.authenticator_class = 'ldapauthenticator.LDAPAuthenticator'\n
    \   c.LDAPAuthenticator.server_address = get_config('auth.ldap.server.address')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'server_port', 'auth.ldap.server.port')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'use_ssl', 'auth.ldap.server.ssl')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'allowed_groups', 'auth.ldap.allowedGroups')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'bind_dn_template', 'auth.ldap.dn.templates')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn', 'auth.ldap.dn.lookup')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_filter', 'auth.ldap.dn.search.filter')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_user', 'auth.ldap.dn.search.user')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_password', 'auth.ldap.dn.search.password')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_user_dn_attribute', 'auth.ldap.dn.user.dnAttribute')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'escape_userdn', 'auth.ldap.dn.user.escape')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'valid_username_regex', 'auth.ldap.dn.user.validRegex')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'user_search_base', 'auth.ldap.dn.user.searchBase')\n
    \   set_config_if_not_none(c.LDAPAuthenticator, 'user_attribute', 'auth.ldap.dn.user.attribute')\nelif
    auth_type == 'custom':\n    # full_class_name looks like \"myauthenticator.MyAuthenticator\".\n
    \   # To create a docker image with this class availabe, you can just have the\n
    \   # following Dockerfile:\n    #   FROM jupyterhub/k8s-hub:v0.4\n    #   RUN pip3
    install myauthenticator\n    full_class_name = get_config('auth.custom.className')\n
    \   c.JupyterHub.authenticator_class = full_class_name\n    auth_class_name = full_class_name.rsplit('.',
    1)[-1]\n    auth_config = c[auth_class_name]\n    auth_config.update(get_config('auth.custom.config')
    or {})\nelse:\n    raise ValueError(\"Unhandled auth type: %r\" % auth_type)\n\nset_config_if_not_none(c.OAuthenticator,
    'scope', 'auth.scopes')\n\nset_config_if_not_none(c.Authenticator, 'enable_auth_state',
    'auth.state.enabled')\n\n# Enable admins to access user servers\nset_config_if_not_none(c.JupyterHub,
    'admin_access', 'auth.admin.access')\nset_config_if_not_none(c.Authenticator, 'admin_users',
    'auth.admin.users')\nset_config_if_not_none(c.Authenticator, 'whitelist', 'auth.whitelist.users')\n\nc.JupyterHub.services
    = []\n\nif get_config('cull.enabled', False):\n    cull_cmd = [\n        'python3',\n
    \       '/etc/jupyterhub/cull_idle_servers.py',\n    ]\n    base_url = c.JupyterHub.get('base_url',
    '/')\n    cull_cmd.append(\n        '--url=http://127.0.0.1:8081' + url_path_join(base_url,
    'hub/api')\n    )\n\n    cull_timeout = get_config('cull.timeout')\n    if cull_timeout:\n
    \       cull_cmd.append('--timeout=%s' % cull_timeout)\n\n    cull_every = get_config('cull.every')\n
    \   if cull_every:\n        cull_cmd.append('--cull-every=%s' % cull_every)\n\n
    \   cull_concurrency = get_config('cull.concurrency')\n    if cull_concurrency:\n
    \       cull_cmd.append('--concurrency=%s' % cull_concurrency)\n\n    if get_config('cull.users'):\n
    \       cull_cmd.append('--cull-users')\n\n    if get_config('cull.removeNamedServers'):\n
    \       cull_cmd.append('--remove-named-servers')\n\n    cull_max_age = get_config('cull.maxAge')\n
    \   if cull_max_age:\n        cull_cmd.append('--max-age=%s' % cull_max_age)\n\n
    \   c.JupyterHub.services.append({\n        'name': 'cull-idle',\n        'admin':
    True,\n        'command': cull_cmd,\n    })\n\nfor name, service in get_config('hub.services',
    {}).items():\n    # jupyterhub.services is a list of dicts, but\n    # in the helm
    chart it is a dict of dicts for easier merged-config\n    service.setdefault('name',
    name)\n    # handle camelCase->snake_case of api_token\n    api_token = service.pop('apiToken',
    None)\n    if api_token:\n        service['api_token'] = api_token\n    c.JupyterHub.services.append(service)\n\n\nset_config_if_not_none(c.Spawner,
    'cmd', 'singleuser.cmd')\nset_config_if_not_none(c.Spawner, 'default_url', 'singleuser.defaultUrl')\n\ncloud_metadata
    = get_config('singleuser.cloudMetadata', {})\n\nif not cloud_metadata.get('enabled',
    False):\n    # Use iptables to block access to cloud metadata by default\n    network_tools_image_name
    = get_config('singleuser.networkTools.image.name')\n    network_tools_image_tag
    = get_config('singleuser.networkTools.image.tag')\n    ip_block_container = client.V1Container(\n
    \       name=\"block-cloud-metadata\",\n        image=f\"{network_tools_image_name}:{network_tools_image_tag}\",\n
    \       command=[\n            'iptables',\n            '-A', 'OUTPUT',\n            '-d',
    cloud_metadata.get('ip', '169.254.169.254'),\n            '-j', 'DROP'\n        ],\n
    \       security_context=client.V1SecurityContext(\n            privileged=True,\n
    \           run_as_user=0,\n            capabilities=client.V1Capabilities(add=['NET_ADMIN'])\n
    \       )\n    )\n\n    c.KubeSpawner.init_containers.append(ip_block_container)\n\n\nif
    get_config('debug.enabled', False):\n    c.JupyterHub.log_level = 'DEBUG'\n    c.Spawner.debug
    = True\n\n\nextra_config = get_config('hub.extraConfig', {})\nif isinstance(extra_config,
    str):\n    from textwrap import indent, dedent\n    msg = dedent(\n    \"\"\"\n
    \   hub.extraConfig should be a dict of strings,\n    but found a single string
    instead.\n\n    extraConfig as a single string is deprecated\n    as of the jupyterhub
    chart version 0.6.\n\n    The keys can be anything identifying the\n    block of
    extra configuration.\n\n    Try this instead:\n\n        hub:\n          extraConfig:\n
    \           myConfig: |\n              {}\n\n    This configuration will still be
    loaded,\n    but you are encouraged to adopt the nested form\n    which enables
    easier merging of multiple extra configurations.\n    \"\"\"\n    )\n    print(\n
    \       msg.format(\n            indent(extra_config, ' ' * 10).lstrip()\n        ),\n
    \       file=sys.stderr\n    )\n    extra_config = {'deprecated string': extra_config}\n\nfor
    key, config_py in sorted(extra_config.items()):\n    print(\"Loading extra config:
    %s\" % key)\n    exec(config_py)\n"
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.
  
    Methods here can be imported by extraConfig in values.yaml
    """
    from collections import Mapping
    from functools import lru_cache
    import os
  
    import yaml
  
  
    # memoize so we only load config once
    @lru_cache()
    def _load_config():
        """Load configuration from disk
  
        Memoized to only load once
        """
        cfg = {}
        for source in ('config', 'secret'):
            path = f"/etc/jupyterhub/{source}/values.yaml"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg
  
  
    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.
  
        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged
  
  
    def get_config(key, default=None):
        """
        Find a config item of a given name & return it
  
        Parses everything as YAML, so lists and dicts are available too
  
        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split('.'):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value
  
  
    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/autohttps/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: traefik-proxy-config
  labels:
    component: autohttps
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
data:
  # This configmap provides the complete configuration for our traefik proxy.
  # traefik has 'static' config and 'dynamic' config (https://docs.traefik.io/getting-started/configuration-overview/)
  # traefik.toml contains the static config, while dynamic.toml has the dynamic config.
  traefik.toml: |

    # We wanna listen on both port 80 & 443
    [entryPoints]
      [entryPoints.http]
        # traefik is used only for TLS termination, so port 80 is just for redirects
        # No configuration for timeouts, etc needed
        address = ":80"

      [entryPoints.https]
        address = ":443"

        [entryPoints.https.transport.respondingTimeouts]
          # High idle timeout, because we have websockets
          idleTimeout = "10m0s"

    [log]
      level = "INFO"

    [accessLog]
      [accessLog.filters]
      # Only error codes
      statusCodes = ["500-599"]

      # Redact possible sensitive headers from log
      [accessLog.fields.headers]
        [accessLog.fields.headers.names]
          Authorization = "redact"
          Cookie = "redact"
          Set-Cookie = "redact"
          X-Xsrftoken = "redact"

    # We want certificates to come from Let's Encrypt, with the HTTP-01 challenge
    [certificatesResolvers.le.acme]
      email = "yuvipanda@gmail.com"
      storage = "/etc/acme/acme.json"
      [certificatesResolvers.le.acme.httpChallenge]
        # Use our port 80 http endpoint for the HTTP-01 challenge
        entryPoint = "http"

    [providers]
      [providers.file]
        # Configuration for routers & other dynamic items come from this file
        # This file is also provided by this configmap
        filename = '/etc/traefik/dynamic.toml'

  dynamic.toml: |
    # Configure TLS to give us an A+ in the ssllabs test
    [tls]
      [tls.options]
        [tls.options.default]
          sniStrict = true
          # Do not support insecureTLS 1.0 or 1.1
          minVersion = "VersionTLS12"
          # Ciphers to support, adapted from https://ssl-config.mozilla.org/#server=traefik&server-version=1.7.12&config=intermediate
          # This supports a reasonable number of browsers.
          cipherSuites = [
            "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
            "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
            "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
            "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
            "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
            "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
          ]

    # traefik uses middlewares to set options in specific routes
    # These set up the middleware options, which are then referred to by the routes
    [http.middlewares]
      # Used to do http -> https redirects
      [http.middlewares.redirect.redirectScheme]
        scheme = "https"

      # Used to set appropriate headers in requests sent to CHP
      [http.middlewares.https.headers]
        [http.middlewares.https.headers.customRequestHeaders]
          # Tornado needs this for referrer checks
          # You run into stuff like https://github.com/jupyterhub/jupyterhub/issues/2284 otherwise
          X-Scheme = "https"

      # Used to set HSTS headers based on user provided options
      [http.middlewares.hsts.headers]
        stsSeconds = 15724800
        


    # Routers match conditions (rule) to options (middlewares) and a backend (service)
    [http.routers]
      # Listen on the http endpoint (port 80), redirect everything to https
      [http.routers.httpredirect]
        rule = "PathPrefix(`/`)"
        service = "chp"
        entrypoints = ["http"]
        middlewares = ["redirect"]

      # Listen on https endpoint (port 443), proxy everything to chp
      [http.routers.chp]
        rule = "PathPrefix(`/`)"
        entrypoints = ["https"]
        middlewares = ["hsts", "https"]
        service = "chp"

          [http.routers.chp.tls]
            # use our nice TLS defaults, and get HTTPS from Let's Encrypt
            options = "default"
            certResolver = "le"
            [[http.routers.chp.tls.domains]]
            main = "staging.farallon.2i2c.cloud"

    # Set CHP to be our only backend where traffic is routed to
    [http.services]
      [http.services.chp.loadBalancer]
        [[http.services.chp.loadBalancer.servers]]
          url = "http://proxy-http:8000/"
---
# Source: hub/templates/jupyter-notebook-config.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: user-etc-jupyter
  labels:
    app: jupyterhub
    component: etc-jupyter
    heritage: Helm
    release: farallon-staging
data:
---
# Source: hub/templates/nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: farallon-staging-home-nfs
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: "fs-7b129903.efs.us-east-2.amazonaws.com"
    path: "/"
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: hub-db-dir
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "1Gi"
---
# Source: hub/templates/nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: home-nfs
spec:
  accessModes:
    - ReadWriteMany
  # Match name of PV
  volumeName: farallon-staging-home-nfs
  storageClassName: ""
  resources:
    requests:
      storage: 1Mi  # This does not matter
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/controller/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: controller-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
rules:
  - apiGroups: ["gateway.dask.org"]
    resources: ["daskclusters", "daskclusters/status"]
    verbs: ["*"]
  - apiGroups: ["traefik.containo.us"]
    resources: ["ingressroutes", "ingressroutetcps"]
    verbs: ["get", "create", "delete"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["secrets", "services"]
    verbs: ["create", "delete"]
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
  - apiGroups: ["gateway.dask.org"]
    resources: ["daskclusters"]
    verbs: ["*"]
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/traefik/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: traefik-farallon-staging-dask-gateway
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - traefik.containo.us
    resources:
      - ingressroutes
      - ingressroutetcps
      - middlewares
      - tlsoptions
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/controller/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: controller-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
subjects:
  - kind: ServiceAccount
    name: controller-farallon-staging-dask-gateway
    namespace: farallon-staging
roleRef:
  kind: ClusterRole
  name: controller-farallon-staging-dask-gateway
  apiGroup: rbac.authorization.k8s.io
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
subjects:
  - kind: ServiceAccount
    name: api-farallon-staging-dask-gateway
    namespace: farallon-staging
roleRef:
  kind: ClusterRole
  name: api-farallon-staging-dask-gateway
  apiGroup: rbac.authorization.k8s.io
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/traefik/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: traefik-farallon-staging-dask-gateway
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-farallon-staging-dask-gateway
subjects:
- kind: ServiceAccount
  name: traefik-farallon-staging-dask-gateway
  namespace: farallon-staging
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
rules:
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["pods", "persistentvolumeclaims"]
    verbs: ["get", "watch", "list", "create", "delete"]
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["events"]
    verbs: ["get", "watch", "list"]
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/autohttps/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: autohttps
  labels:
    component: autohttps
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "patch", "list", "create"]
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: hub
    namespace: farallon-staging
roleRef:
  kind: Role
  name: hub
  apiGroup: rbac.authorization.k8s.io
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/autohttps/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: autohttps
  labels:
    component: autohttps
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
- kind: ServiceAccount
  name: autohttps
  apiGroup:
roleRef:
  kind: Role
  name: autohttps
  apiGroup: rbac.authorization.k8s.io
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/component: gateway
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/traefik/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: traefik-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: dask-gateway
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/component: traefik
  ports:
    - name: web
      targetPort: 8000
      port: 80
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /hub/metrics
spec:
  type: ClusterIP
  selector:
    component: hub
    app: jupyterhub
    release: farallon-staging
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/autohttps/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-http
  labels:
    component: autohttps
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
  annotations: {}
spec:
  type: ClusterIP
  selector:
    component: proxy
    app: jupyterhub
    release: farallon-staging
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-api
  labels:
    component: proxy-api
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    component: proxy
    app: jupyterhub
    release: farallon-staging
  ports:
    - protocol: TCP
      port: 8001
      targetPort: 8001
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-public
  labels:
    component: proxy-public
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    # TODO: Refactor to utilize the helpers
    component: autohttps
    release: farallon-staging
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
      # allow proxy.service.nodePort for http
  type: LoadBalancer
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/controller/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: dask-gateway
      app.kubernetes.io/instance: farallon-staging
      app.kubernetes.io/component: controller
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dask-gateway
        helm.sh/chart: dask-gateway-0.8.0
        app.kubernetes.io/instance: farallon-staging
        app.kubernetes.io/version: "0.8.0"
        app.kubernetes.io/managed-by: Helm
        gateway.dask.org/instance: farallon-staging-dask-gateway
        app.kubernetes.io/component: controller
      annotations:
        checksum/configmap: 994987c286235398d9222e62546e708bdca8751a1558e2e11335cb2b710d15ce
    spec:
      serviceAccountName: controller-farallon-staging-dask-gateway
      volumes:
        - name: configmap
          configMap:
            name: controller-farallon-staging-dask-gateway
      containers:
        - name: controller
          image: daskgateway/dask-gateway-server:0.8.0
          imagePullPolicy: IfNotPresent
          command:
            - dask-gateway-server
            - kube-controller
            - --config
            - /etc/dask-gateway/dask_gateway_config.py
          resources:
            {}
          volumeMounts:
            - mountPath: /etc/dask-gateway/
              name: configmap
          ports:
            - containerPort: 8000
              name: api
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dask-gateway
      app.kubernetes.io/instance: farallon-staging
      app.kubernetes.io/component: gateway
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dask-gateway
        helm.sh/chart: dask-gateway-0.8.0
        app.kubernetes.io/instance: farallon-staging
        app.kubernetes.io/version: "0.8.0"
        app.kubernetes.io/managed-by: Helm
        gateway.dask.org/instance: farallon-staging-dask-gateway
        app.kubernetes.io/component: gateway
      annotations:
        checksum/configmap: aea9ab1a729f0f322ffd083c4f5ae73519b98cdf8cecca15cfa5cce49ec3aad9
        checksum/secret: 5cc7243afb51c93016df4691f2436244189c46af1acef5f5a540f1540cc1f7f8
    spec:
      serviceAccountName: api-farallon-staging-dask-gateway
      volumes:
        - name: configmap
          configMap:
            name: api-farallon-staging-dask-gateway
      containers:
        - name: gateway
          image: daskgateway/dask-gateway-server:0.8.0
          imagePullPolicy: IfNotPresent
          command:
            - dask-gateway-server
            - --config
            - /etc/dask-gateway/dask_gateway_config.py
          resources:
            {}
          volumeMounts:
            - mountPath: /etc/dask-gateway/
              name: configmap
          env:
            - name: JUPYTERHUB_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: api-farallon-staging-dask-gateway
                  key: jupyterhub-api-token
          ports:
            - containerPort: 8000
              name: api
          livenessProbe:
            httpGet:
              path: /api/health
              port: api
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /api/health
              port: api
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 3
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/traefik/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traefik-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dask-gateway
      app.kubernetes.io/instance: farallon-staging
      app.kubernetes.io/component: traefik
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dask-gateway
        helm.sh/chart: dask-gateway-0.8.0
        app.kubernetes.io/instance: farallon-staging
        app.kubernetes.io/version: "0.8.0"
        app.kubernetes.io/managed-by: Helm
        gateway.dask.org/instance: farallon-staging-dask-gateway
        app.kubernetes.io/component: traefik
    spec:
      serviceAccountName: traefik-farallon-staging-dask-gateway
      terminationGracePeriodSeconds: 60
      containers:
        - name: traefik
          image: traefik:2.1.3
          securityContext:
            runAsUser: 1000
            runAsGroup: 1000
          resources:
            {}
          args:
            - "--global.checknewversion=False"
            - "--global.sendanonymoususage=False"
            - "--ping=true"
            - "--providers.kubernetescrd"
            - '--providers.kubernetescrd.labelselector=gateway.dask.org/instance=farallon-staging-dask-gateway'
            - "--providers.kubernetescrd.throttleduration=2"
            - "--log.level=WARN"
            - "--entryPoints.traefik.address=:9000"
            - "--entryPoints.web.address=:8000"
          ports:
            - name: traefik
              containerPort: 9000
            - name: web
              containerPort: 8000
          readinessProbe:
            httpGet:
              path: /ping
              port: 9000
            failureThreshold: 1
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          livenessProbe:
            httpGet:
              path: /ping
              port: 9000
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/hub/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: farallon-staging
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: hub
        app: jupyterhub
        release: farallon-staging
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # This lets us autorestart when the secret changes!
        checksum/config-map: fcacb773e9b09f3f57870eea42c8a2b3ef0047b04d8b67c805ff2214ea0e6da6
        checksum/secret: 530c1b2f347163ee47b3d18819bfe34f7bb29dc14721c15fa8c618f00d0c3db5
    spec:
      priorityClassName: farallon-staging-default-priority
      nodeSelector: {"jupyterhub-pool-name":"core-pool"}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      volumes:
        - name: config
          configMap:
            name: hub-config
        - name: secret
          secret:
            secretName: hub-secret
        - emptyDir: {}
          name: custom-templates
        - name: hub-db-dir
          persistentVolumeClaim:
            claimName: hub-db-dir
      serviceAccountName: hub
      securityContext:
        fsGroup: 1000
      initContainers:
        - args:
          - clone
          - --depth=1
          - --single-branch
          - --
          - https://github.com/2i2c-utoronto/homepage
          - /srv/repo
          image: alpine/git
          name: templates-clone
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1000
          volumeMounts:
          - mountPath: /srv/repo
            name: custom-templates
      containers:
        - args:
          - -c
          - while true; do git fetch origin; git reset --hard origin/master; sleep 5m; done
          command:
          - /bin/sh
          image: alpine/git
          name: templates-sync
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1000
          volumeMounts:
          - mountPath: /srv/repo
            name: custom-templates
          workingDir: /srv/repo
        - name: hub
          image: jupyterhub/k8s-hub:0.9.1
          command:
            - jupyterhub
            - --config
            - /etc/jupyterhub/jupyterhub_config.py
            - --upgrade-db
          volumeMounts:
            - mountPath: /etc/jupyterhub/jupyterhub_config.py
              subPath: jupyterhub_config.py
              name: config
            - mountPath: /etc/jupyterhub/z2jh.py
              subPath: z2jh.py
              name: config
            - mountPath: /etc/jupyterhub/cull_idle_servers.py
              subPath: cull_idle_servers.py
              name: config
            - mountPath: /etc/jupyterhub/config/
              name: config
            - mountPath: /etc/jupyterhub/secret/
              name: secret
            - mountPath: /usr/local/share/jupyterhub/custom_templates
              name: custom-templates
              subPath: templates
            - mountPath: /usr/local/share/jupyterhub/static/extra-assets
              name: custom-templates
              subPath: extra-assets
            - mountPath: /srv/jupyterhub
              name: hub-db-dir
          resources:
            limits:
              memory: 2Gi
            requests:
              cpu: 0.001
              memory: 512Mi
          securityContext:
            runAsUser: 1000
            # Don't allow any process to execute as root inside the container
            allowPrivilegeEscalation: false
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: HELM_RELEASE_NAME
              value: "farallon-staging"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
            - name: JUPYTERHUB_CRYPT_KEY
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: auth.state.crypto-key
          ports:
            - containerPort: 8081
              name: hub
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/autohttps/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: autohttps
  labels:
    component: autohttps
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: autohttps
      app: jupyterhub
      release: farallon-staging
  template:
    metadata:
      labels:
        component: autohttps
        app: jupyterhub
        release: farallon-staging
        hub.jupyter.org/network-access-proxy-http: "true"
      annotations:
        checksum/config-map: b5e993c06ac5d490dfb1d25c34468a4b0709f4bfb6aefb92382a95f35f981f86
    spec:
      serviceAccountName: autohttps
      priorityClassName: farallon-staging-default-priority
      nodeSelector: {"jupyterhub-pool-name":"core-pool"}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      volumes:
        - name: certificates
          emptyDir: {}
        - name: traefik-config
          configMap:
            name: traefik-proxy-config
      initContainers:
      - name: load-acme
        image: "jupyterhub/k8s-secret-sync:0.9.1"
        command: ["/usr/local/bin/secret-sync.py", "load", "proxy-public-tls-acme", "acme.json", "/etc/acme/acme.json"]
        env:
        # We need this to get logs immediately
        - name: PYTHONUNBUFFERED
          value: "True"
        volumeMounts:
          - name: certificates
            mountPath: /etc/acme
      containers:
        - name: traefik
          image: "traefik:v2.1"
          resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 0.001
              memory: 256Mi
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          volumeMounts:
            - name: traefik-config
              mountPath: /etc/traefik
            - name: certificates
              mountPath: /etc/acme
        - name: secret-sync
          image: "jupyterhub/k8s-secret-sync:0.9.1"
          command:
            - /usr/local/bin/secret-sync.py
            - watch-save
            - --label=app=jupyterhub
            - --label=release=farallon-staging
            - --label=chart=jupyterhub-0.9.1
            - --label=heritage=secret-sync
            - proxy-public-tls-acme
            - acme.json
            - /etc/acme/acme.json
          env:
          # We need this to get logs immediately
          - name: PYTHONUNBUFFERED
            value: "True"
          volumeMounts:
            - name: certificates
              mountPath: /etc/acme
---
# Source: hub/charts/daskhub/charts/jupyterhub/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: farallon-staging
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: farallon-staging
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: proxy
        app: jupyterhub
        release: farallon-staging
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # This lets us autorestart when the secret changes!
        checksum/hub-secret: a0bf8aed07175b1986e6db050f91499b045ffe3dea92e25e448d76a3e94a3870
        checksum/proxy-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      terminationGracePeriodSeconds: 60
      priorityClassName: farallon-staging-default-priority
      nodeSelector: {"jupyterhub-pool-name":"core-pool"}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      containers:
        - name: chp
          image: jupyterhub/configurable-http-proxy:4.2.1
          command:
            - configurable-http-proxy
            - --ip=0.0.0.0
            - --api-ip=0.0.0.0
            - --api-port=8001
            - --default-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)
            - --error-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)/hub/error
            - --port=8000
          resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 0.001
              memory: 128Mi
          securityContext:
            # Don't allow any process to execute as root inside the container
            allowPrivilegeEscalation: false
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
          ports:
            - containerPort: 8000
              name: proxy-public
            - containerPort: 8001
              name: api
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            httpGet:
              path: /_chp_healthz
              port: proxy-public
              scheme: HTTP
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 10
            httpGet:
              path: /_chp_healthz
              port: proxy-public
              scheme: HTTP
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/ingressroute.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: api-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  entryPoints:
    - web
  routes:
  - match: PathPrefix(`/services/dask-gateway`)
    kind: Rule
    services:
    - name: api-farallon-staging-dask-gateway
      port: 8000
    middlewares:
    - name: 'api-prefix-farallon-staging-dask-gateway'
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/middleware.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: clusters-prefix-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  stripPrefixRegex:
    regex:
      - '/services/dask-gateway/clusters/[a-zA-Z0-9.-]+'
---
# Source: hub/charts/daskhub/charts/dask-gateway/templates/gateway/middleware.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: api-prefix-farallon-staging-dask-gateway
  labels:
    app.kubernetes.io/name: dask-gateway
    helm.sh/chart: dask-gateway-0.8.0
    app.kubernetes.io/instance: farallon-staging
    app.kubernetes.io/version: "0.8.0"
    app.kubernetes.io/managed-by: Helm
    gateway.dask.org/instance: farallon-staging-dask-gateway
spec:
  stripPrefix:
    prefixes:
      - '/services/dask-gateway'
    forceSlash: false

